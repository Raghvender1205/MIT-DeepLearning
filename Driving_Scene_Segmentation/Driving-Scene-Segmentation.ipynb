{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)\n\n# I/O Libraries\nimport os\nfrom io import BytesIO\nimport tarfile\nimport tempfile\nfrom six.moves import urllib\n\nimport matplotlib\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\nimport cv2\nfrom tqdm import tqdm\nfrom sklearn.metrics import confusion_matrix\nfrom tabulate import tabulate\n\nimport warnings\nwarnings.simplefilter('ignore', DeprecationWarning)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-15T21:40:01.465675Z","iopub.execute_input":"2022-01-15T21:40:01.466469Z","iopub.status.idle":"2022-01-15T21:40:06.603219Z","shell.execute_reply.started":"2022-01-15T21:40:01.466370Z","shell.execute_reply":"2022-01-15T21:40:06.602491Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Build Model\n`DeepLab` is a DeepLearning model for semantic image segmentation, where the goal is to assign semantic labels to every pixel in the input image.\n<img src=\"https://camo.githubusercontent.com/557038ad2a63a106c4ca97b82cae48e345c2c017f8b46db291600c31e67569e0/68747470733a2f2f6769746875622e636f6d2f74656e736f72666c6f772f6d6f64656c732f626c6f622f6d61737465722f72657365617263682f646565706c61622f6733646f632f696d672f766973322e706e673f7261773d74727565\"/>\n<img src=\"https://camo.githubusercontent.com/1c93625310141758671743ffac614750d3d324dcd1beef50075cd02b12d6459d/68747470733a2f2f6769746875622e636f6d2f74656e736f72666c6f772f6d6f64656c732f626c6f622f6d61737465722f72657365617263682f646565706c61622f6733646f632f696d672f766973312e706e673f7261773d74727565\"/>\n\nIn the driving context, we aim to obtain a semantic understanding of the front driving scene throught the camera input. This is important for driving safety and an essential requirement for all levels of autonomous driving. The first step is to build the model and load the pre-trained weights. In this demo, we use the model checkpoint trained on `Cityscapes` dataset.\n<img src=\"https://camo.githubusercontent.com/fa58d4500df52b272cde52df3d146099350286f5eea9dcf5b43965ee02d487b4/68747470733a2f2f7777772e636974797363617065732d646174617365742e636f6d2f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031352f30372f6d75656e7374657230302e706e67\"/>\n<img src=\"https://camo.githubusercontent.com/8b211a1ff514143ea596ab37290fc80f75658f2157d3641da8f38b46c031d402/68747470733a2f2f7777772e636974797363617065732d646174617365742e636f6d2f776f726470726573732f77702d636f6e74656e742f75706c6f6164732f323031352f30372f7a75657269636830302e706e67\"/>","metadata":{}},{"cell_type":"code","source":"# tf.compat.v1.disable_eager_execution()\n# tf.compat.v1.disable_v2_behavior()\n\nclass DeepLab(object):\n    \"\"\" Class to load deeplab model and run inference \"\"\"\n    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n    \n    def __init__(self, tarball_path):\n        \"\"\" Creates and loads pretrained deeplab model. \"\"\"\n        self.graph = tf.compat.v1.Graph()\n        graph_def = None\n        \n        # Extract frozen graph from tar archive\n        tar_file = tarfile.open(tarball_path)\n        for tar_info in tar_file.getmembers():\n            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n                file_handle = tar_file.extractfile(tar_info)\n                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n                break\n        tar_file.close()\n        \n        if graph_def is None:\n            raise RuntimeError('Cannot find inference graph in tar archive.')\n\n        with self.graph.as_default():\n            tf.compat.v1.import_graph_def(graph_def, name='')\n        self.sess = tf.compat.v1.Session(graph=self.graph)\n        \n    def run(self, image, INPUT_TENSOR_NAME='ImageTensor:0', OUTPUT_TENSOR_NAME='SemanticPredictions:0'):\n        \"\"\"\n        Runs Inference on a single image\n        \n        Args:\n            image: PIL.Image object, raw input image\n            INPUT_TENSOR_IMAGE: Name of input tensor, default to ImageTensor.\n            OUTPUT_TENSOR_IMAGE: Name of output tensor, default to SemanticPredictions.\n            \n        Returns:\n            resized_image: RGB image resized from original input image.\n            seg_map: Segmentation map of `resized_image`\n        \"\"\"\n        width, height = image.size\n        target_size = (2049, 1025) # Size of Cityscapes images\n        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n        batch_seg_map = self.sess.run(OUTPUT_TENSOR_NAME, feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n        seg_map = batch_seg_map[0] # Expected Batch Size = 1\n        \n        if len(seg_map.shape) == 2:\n            seg_map = np.expand_dims(seg_map, -1) # Need an Extra Dimension for cv2.resize\n        seg_map = cv2.resize(seg_map, (width, height), interpolation=cv2.INTER_NEAREST)\n        return seg_map","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:40:06.604760Z","iopub.execute_input":"2022-01-15T21:40:06.605326Z","iopub.status.idle":"2022-01-15T21:40:06.617873Z","shell.execute_reply.started":"2022-01-15T21:40:06.605285Z","shell.execute_reply":"2022-01-15T21:40:06.617033Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Visualization\nCreate Helper functions for decoding and visualizing results","metadata":{}},{"cell_type":"code","source":"def create_label_colormap():\n    \"\"\"\n    Creates a label colormap used in CityScapes benchmark\n    \n    Returns:\n        A Colormap for visualizing segmentation results\n    \"\"\"\n    colormap = np.array([\n        [128,  64, 128],\n        [244,  35, 232],\n        [ 70,  70,  70],\n        [102, 102, 156],\n        [190, 153, 153],\n        [153, 153, 153],\n        [250, 170,  30],\n        [220, 220,   0],\n        [107, 142,  35],\n        [152, 251, 152],\n        [ 70, 130, 180],\n        [220,  20,  60],\n        [255,   0,   0],\n        [  0,   0, 142],\n        [  0,   0,  70],\n        [  0,  60, 100],\n        [  0,  80, 100],\n        [  0,   0, 230],\n        [119,  11,  32],\n        [  0,   0,   0]], dtype=np.uint8)\n    return colormap\n\n\ndef label_to_color_image(label):\n    \"\"\"Adds color defined by the dataset colormap to the label.\n\n    Args:\n        label: A 2D array with integer type, storing the segmentation label.\n\n    Returns:\n        result: A 2D array with floating type. The element of the array\n            is the color indexed by the corresponding element in the input label\n            to the PASCAL color map.\n\n    Raises:\n        ValueError: If label is not of rank 2 or its value is larger than color\n            map maximum entry.\n    \"\"\"\n    if label.ndim != 2:\n        raise ValueError('Expect 2-D input label')\n\n    colormap = create_label_colormap()\n\n    if np.max(label) >= len(colormap):\n        raise ValueError('label value too large.')\n\n    return colormap[label]\n\n\ndef vis_segmentation(image, seg_map):\n    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n    plt.figure(figsize=(20, 4))\n    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1])\n\n    plt.subplot(grid_spec[0])\n    plt.imshow(image)\n    plt.axis('off')\n    plt.title('input image')\n\n    plt.subplot(grid_spec[1])\n    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n    plt.imshow(seg_image)\n    plt.axis('off')\n    plt.title('segmentation map')\n\n    plt.subplot(grid_spec[2])\n    plt.imshow(image)\n    plt.imshow(seg_image, alpha=0.7)\n    plt.axis('off')\n    plt.title('segmentation overlay')\n\n    unique_labels = np.unique(seg_map)\n    ax = plt.subplot(grid_spec[3])\n    plt.imshow(FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n    ax.yaxis.tick_right()\n    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n    plt.xticks([], [])\n    ax.tick_params(width=0.0)\n    plt.grid('off')\n    plt.show()\n\n\nLABEL_NAMES = np.asarray([\n    'road', 'sidewalk', 'building', 'wall', 'fence', 'pole', 'traffic light',\n    'traffic sign', 'vegetation', 'terrain', 'sky', 'person', 'rider', 'car', 'truck',\n    'bus', 'train', 'motorcycle', 'bicycle', 'void'])\n\nFULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\nFULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:40:07.867216Z","iopub.execute_input":"2022-01-15T21:40:07.867492Z","iopub.status.idle":"2022-01-15T21:40:07.885895Z","shell.execute_reply.started":"2022-01-15T21:40:07.867462Z","shell.execute_reply":"2022-01-15T21:40:07.885222Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Load Model from a Frozen Graph\nThere are 2 model checkpoints pretrained on CityScapes with different network backbones: `MobileNetV2` and `Xception65`. We will use `MobileNetV2` for inference.","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n\n_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n_MODEL_URLS = {\n    'mobilenetv2_coco_cityscapes_trainfine':\n        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n    'xception65_cityscapes_trainfine':\n        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n}\n_TARBALL_NAME = 'deeplab_model.tar.gz'\n\nmodel_dir = tempfile.mkdtemp()\ntf.compat.v1.gfile.MakeDirs(model_dir)\n\ndownload_path = os.path.join(model_dir, _TARBALL_NAME)\nprint('Downloading model, this might take a while...')\nurllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\nprint('Download Completed!, loading DeepLab Model...')\n\nMODEL = DeepLab(download_path)\nprint('Model loaded Successfully!!')","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:40:11.302457Z","iopub.execute_input":"2022-01-15T21:40:11.302723Z","iopub.status.idle":"2022-01-15T21:40:14.662279Z","shell.execute_reply.started":"2022-01-15T21:40:11.302694Z","shell.execute_reply":"2022-01-15T21:40:14.661551Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Run on Sample Image\nThe sample image is frame#0 in Dataset `(mitdrivingsegmentation)`","metadata":{}},{"cell_type":"code","source":"SAMPLE_IMAGE = '../input/mitdrivingsegmentation/mit_driveseg_sample.png'\n\n\ndef visualize(SAMPLE_IMAGE):\n    \"\"\" Inferences DeepLab model and visualizes result. \"\"\"\n    original_im = Image.open(SAMPLE_IMAGE)\n    seg_map = MODEL.run(original_im)\n    vis_segmentation(original_im, seg_map)\n\nvisualize(SAMPLE_IMAGE)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:40:14.663951Z","iopub.execute_input":"2022-01-15T21:40:14.664396Z","iopub.status.idle":"2022-01-15T21:40:23.047281Z","shell.execute_reply.started":"2022-01-15T21:40:14.664352Z","shell.execute_reply":"2022-01-15T21:40:23.045912Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Run on Sample Video","metadata":{}},{"cell_type":"code","source":"import IPython\n\ndef visualize_segmentation_stream(image, seg_map, index):\n    \"\"\" Visualizes Segmentation overlay view and stream it with IPython display \"\"\"\n    plt.figure(figsize=(12, 7))\n\n    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n    plt.imshow(image)\n    plt.imshow(seg_image, alpha=0.7)\n    plt.axis('off')\n    plt.title('segmentation overlay | frame #%d'%index)\n    plt.grid('off')\n    plt.tight_layout()\n\n    # Show visualization in a streaming fashion.\n    f = BytesIO()\n    plt.savefig(f, format='jpeg')\n    IPython.display.display(IPython.display.Image(data=f.getvalue()))\n    f.close()\n    plt.close()\n    \ndef run_visualization_video(frame, index):\n    \"\"\" Inferences DeepLab Model on a Video File and Stream the Visualization \"\"\"\n    original_im = Image.fromarray(frame[..., ::-1])\n    seg_map = MODEL.run(original_im)../input/mitdrivingsegmentation/lize_segmentation_stream(original_im, seg_map, index)\n    \nSAMPLE_VIDEO = '../input/mitdrivingsegmentation/mit_driveseg_sample.mp4'\n# Capture Video Frames\nvideo = cv2.VideoCapture(SAMPLE_VIDEO)\nnum_frames = 30\n\ntry:\n    for i in range(num_frames):\n        _, frame = video.read()\n        if not _: break\n        run_visualization_video(frame, i)\n        IPython.display.clear_output(wait=True)\nexcept KeyboardInterrupt:\n    plt.close()\n    print(\"Stream stopped.\")","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:46:36.652835Z","iopub.execute_input":"2022-01-15T21:46:36.653312Z","iopub.status.idle":"2022-01-15T21:46:53.930800Z","shell.execute_reply.started":"2022-01-15T21:46:36.653267Z","shell.execute_reply":"2022-01-15T21:46:53.930086Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation\nLet's evaluate ","metadata":{}},{"cell_type":"code","source":"class DriveSeg(object):\n    \"\"\"Class to load MIT DriveSeg Dataset.\"\"\"\n\n    def __init__(self, tarball_path):\n        self.tar_file = tarfile.open(tarball_path)\n        self.tar_info = self.tar_file.getmembers()\n    \n    def fetch(self, index):\n        \"\"\"Get ground truth by index.\n\n        Args:\n            index: The frame number.\n\n        Returns:\n            gt: Ground truth segmentation map.\n        \"\"\"\n        tar_info = self.tar_info[index + 1]  # exclude index 0 which is the parent directory\n        file_handle = self.tar_file.extractfile(tar_info)\n        gt = np.fromstring(file_handle.read(), np.uint8)\n        gt = cv2.imdecode(gt, cv2.IMREAD_COLOR)\n        gt = gt[:, :, 0]  # select a single channel from the 3-channel image\n        gt[gt==255] = 19  # void class, does not count for accuracy\n        return gt\n\n\nSAMPLE_GT = 'mit_driveseg_sample_gt.tar.gz'\nif not os.path.isfile(SAMPLE_GT): \n    print('downloading the sample ground truth...')\n    SAMPLE_GT = urllib.request.urlretrieve('https://github.com/lexfridman/mit-deep-learning/raw/master/tutorial_driving_scene_segmentation/mit_driveseg_sample_gt.tar.gz')[0]\n\ndataset = DriveSeg(SAMPLE_GT)\nprint('visualizing ground truth annotation on the sample image...')\n\noriginal_im = Image.open(SAMPLE_IMAGE)\ngt = dataset.fetch(0)  # sample image is frame 0\nvis_segmentation(original_im, gt)","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:56:55.523118Z","iopub.execute_input":"2022-01-15T21:56:55.523372Z","iopub.status.idle":"2022-01-15T21:56:57.345381Z","shell.execute_reply.started":"2022-01-15T21:56:55.523341Z","shell.execute_reply":"2022-01-15T21:56:57.344757Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation on Sample Image\nThere are many ways to measure the performance of a segmentation model. The most straight forward one is pixel accuracy, which calculates how many pixels are correctly predicted. Another commonly used one is the standard `Jaccard Index` (intersection-over-union) as `IoU = TP ‚ÅÑ (TP+FP+FN)`, where TP, FP, and FN are the numbers of true positive, false positive, and false negative pixels, respectively.","metadata":{}},{"cell_type":"code","source":"def evaluate_single(seg_map, ground_truth):\n    \"\"\"Evaluate a single frame with the MODEL loaded.\"\"\"    \n    # merge label due to different annotation scheme\n    seg_map[np.logical_or(seg_map==14,seg_map==15)] = 13\n    seg_map[np.logical_or(seg_map==3,seg_map==4)] = 2\n    seg_map[seg_map==12] = 11\n\n    # calculate accuracy on valid area\n    acc = np.sum(seg_map[ground_truth!=19]==ground_truth[ground_truth!=19])/np.sum(ground_truth!=19)\n    \n    # select valid labels for evaluation\n    cm = confusion_matrix(ground_truth[ground_truth!=19], seg_map[ground_truth!=19], \n                          labels=np.array([0,1,2,5,6,7,8,9,11,13]))\n    intersection = np.diag(cm)\n    union = np.sum(cm, 0) + np.sum(cm, 1) - np.diag(cm)\n    return acc, intersection, union\n\n\nprint('evaluating on the sample image...')\n\noriginal_im = Image.open(SAMPLE_IMAGE)\nseg_map = MODEL.run(original_im)\ngt = dataset.fetch(0)  # sample image is frame 0\nacc, intersection, union = evaluate_single(seg_map, gt)\nclass_iou = np.round(intersection / union, 5)\nprint('pixel accuracy: %.5f'%acc)\nprint('mean class IoU:', np.mean(class_iou))\nprint('class IoU:')\nprint(tabulate([class_iou], headers=LABEL_NAMES[[0,1,2,5,6,7,8,9,11,13]]))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:58:06.319623Z","iopub.execute_input":"2022-01-15T21:58:06.319916Z","iopub.status.idle":"2022-01-15T21:58:08.872612Z","shell.execute_reply.started":"2022-01-15T21:58:06.319884Z","shell.execute_reply":"2022-01-15T21:58:08.871001Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"### Evaluate on Sample Video","metadata":{}},{"cell_type":"code","source":"print('evaluating on the sample video...', flush=True)\n\nvideo = cv2.VideoCapture(SAMPLE_VIDEO)\n# num_frames = 598  # uncomment to use the full sample video\nnum_frames = 30\n\nacc = []\nintersection = []\nunion = []\n\nfor i in tqdm(range(num_frames)):\n    _, frame = video.read()\n    original_im = Image.fromarray(frame[..., ::-1])\n    seg_map = MODEL.run(original_im)\n    gt = dataset.fetch(i)\n    _acc, _intersection, _union = evaluate_single(seg_map, gt)\n    intersection.append(_intersection)\n    union.append(_union)\n    acc.append(_acc)\n\nclass_iou = np.round(np.sum(intersection, 0) / np.sum(union, 0), 4)\nprint('pixel accuracy: %.4f'%np.mean(acc))\nprint('mean class IoU: %.4f'%np.mean(class_iou))\nprint('class IoU:')\nprint(tabulate([class_iou], headers=LABEL_NAMES[[0,1,2,5,6,7,8,9,11,13]]))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T21:58:48.736301Z","iopub.execute_input":"2022-01-15T21:58:48.736571Z","iopub.status.idle":"2022-01-15T22:00:04.376066Z","shell.execute_reply.started":"2022-01-15T21:58:48.736535Z","shell.execute_reply":"2022-01-15T22:00:04.375349Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### Optional: leverage temporal information\nOne thing makes video scene segmentation different from image segmentation is the availability of previous frames, which contains valuable `temporal information` that may help with perception. The open question is how can we use such temporal information. Let's try combine the prediction of two frames instead of only one frame, making smoother predictions over time.","metadata":{}},{"cell_type":"code","source":"print('Evaluating on the sample video with temporal smoothing...', flush=True)\n\nvideo = cv2.VideoCapture(SAMPLE_VIDEO)\nnum_frames = 598  # uncomment to use the full sample video\n#num_frames = 30\n\nacc = []\nintersection = []\nunion = []\nprev_seg_map_logits = 0\n\nfor i in tqdm(range(num_frames)):\n    _, frame = video.read()\n    original_im = Image.fromarray(frame[..., ::-1])\n    \n    # Get the logits instead of label prediction\n    seg_map_logits = MODEL.run(original_im, OUTPUT_TENSOR_NAME='ResizeBilinear_3:0')\n    \n    # Add previous frame's logits and get the results\n    seg_map = np.argmax(seg_map_logits + prev_seg_map_logits, -1)\n    prev_seg_map_logits = seg_map_logits\n    \n    gt = dataset.fetch(i)\n    _acc, _intersection, _union = evaluate_single(seg_map, gt)\n    intersection.append(_intersection)\n    union.append(_union)\n    acc.append(_acc)\n    \nclass_iou = np.round(np.sum(intersection, 0) / np.sum(union, 0), 4)\nprint('pixel accuracy: %.4f'%np.mean(acc))\nprint('mean class IoU: %.4f'%np.mean(class_iou))\nprint('class IoU:')\nprint(tabulate([class_iou], headers=LABEL_NAMES[[0,1,2,5,6,7,8,9,11,13]]))","metadata":{"execution":{"iopub.status.busy":"2022-01-15T22:00:28.977727Z","iopub.execute_input":"2022-01-15T22:00:28.978290Z","iopub.status.idle":"2022-01-15T22:27:48.547606Z","shell.execute_reply.started":"2022-01-15T22:00:28.978252Z","shell.execute_reply":"2022-01-15T22:27:48.546977Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}