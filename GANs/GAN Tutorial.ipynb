{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"source":["## General Adversial Network (GAN)\n","\n","Generative Adversarial Networks (GANs) are a framework for training networks optimized for generating new realistic samples from a particular representation. In its simplest form, the training process involves two networks. One network, called the `Generator`, generates new data instances, trying to fool the other network, the `Discriminator`, that classifies images as real or fake. \n","<img src=\"https://camo.githubusercontent.com/22afb5278428d6c4c785d1e62639b6dcafc73c7a59b64e86f6d4f71ed54df094/68747470733a2f2f692e696d6775722e636f6d2f4c7765614431732e706e67\"/>\n","\n","There are broadly 3 categories of `GANs`.\n","1. <b>Unsupervised GANs</b>: The Generator network takes random noise as input and produces a photo-realistic image that appears very similar to images that appears in the training dataset. Example `DC-GAN`, `pg-GAN`\n","2. <b>Style-Transfer GANs</b>: Translate images from one domain to another. Examples like `CycleGAN` and `Pix2Pix`\n","3. <b>Conditional GANs</b>: Jointly learn on features along with images to generate images conditioned on those features. Example like `Conditional GAN`, `AC-GAN` and `BigGAN`.\n","\n","### Part 1: BigGAN"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:12:32.716133Z","iopub.status.busy":"2022-01-16T15:12:32.715411Z","iopub.status.idle":"2022-01-16T15:12:43.554363Z","shell.execute_reply":"2022-01-16T15:12:43.553533Z","shell.execute_reply.started":"2022-01-16T15:12:32.716094Z"},"trusted":true},"outputs":[],"source":["# Install imageio-ffmpeg\n","!pip install imageio-ffmpeg"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:12:46.554316Z","iopub.status.busy":"2022-01-16T15:12:46.553825Z","iopub.status.idle":"2022-01-16T15:12:46.566943Z","shell.execute_reply":"2022-01-16T15:12:46.566040Z","shell.execute_reply.started":"2022-01-16T15:12:46.554279Z"},"trusted":true},"outputs":[],"source":["import io\n","import os\n","import numpy as np\n","\n","# Deep Learning\n","from scipy.stats import truncnorm\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","\n","# Visualization\n","from IPython.core.display import HTML\n","import imageio\n","import base64\n","\n","# Checks that TensorFlow GPU is Enabled\n","tf.test.gpu_device_name()   "]},{"cell_type":"markdown","metadata":{},"source":["### Load  BigGAN Generator Module from TFHub "]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:12:48.894348Z","iopub.status.busy":"2022-01-16T15:12:48.893875Z","iopub.status.idle":"2022-01-16T15:12:59.052285Z","shell.execute_reply":"2022-01-16T15:12:59.051532Z","shell.execute_reply.started":"2022-01-16T15:12:48.894309Z"},"trusted":true},"outputs":[],"source":["# Remove EagerExecution in TF2.x for preventing this Error\n","# RuntimeError: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.\n","tf.compat.v1.disable_eager_execution()\n","\n","module_path = 'https://tfhub.dev/deepmind/biggan-512/1' # 512x512 BigGAN\n","\n","tf.compat.v1.reset_default_graph()\n","print('Loading BigGAN module from: ', module_path)\n","module = hub.Module(module_path)\n","inputs = {k: tf.compat.v1.placeholder(v.dtype, v.get_shape().as_list(), k) for k, v in module.get_input_info_dict().items()}\n","output = module(inputs)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:12:59.059819Z","iopub.status.busy":"2022-01-16T15:12:59.059597Z","iopub.status.idle":"2022-01-16T15:12:59.066308Z","shell.execute_reply":"2022-01-16T15:12:59.065469Z","shell.execute_reply.started":"2022-01-16T15:12:59.059791Z"},"trusted":true},"outputs":[],"source":["outputs"]},{"cell_type":"markdown","metadata":{},"source":["### Functions for Sampling and Interpolating the Generator"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:13:02.204933Z","iopub.status.busy":"2022-01-16T15:13:02.204278Z","iopub.status.idle":"2022-01-16T15:13:02.222028Z","shell.execute_reply":"2022-01-16T15:13:02.221323Z","shell.execute_reply.started":"2022-01-16T15:13:02.204893Z"},"trusted":true},"outputs":[],"source":["input_z = inputs['z']\n","input_y = inputs['y']\n","input_trunc = inputs['truncation']\n","\n","dim_z = input_z.shape.as_list()[1]\n","vocab_size = input_y.shape.as_list()[1]\n","\n","# Sample Truncated Normal distribution based on Seed and Truncation Parameter\n","def truncated_z_sample(truncation=1., seed=None):\n","    state = None if seed is None else np.random.RandomState(seed)\n","    values = truncnorm.rvs(-2, 2, size=(1, dim_z), random_state=state)\n","    return truncation * values\n","\n","# Convert `index` value to a vector of all zeros except for a 1 at `index`\n","def one_hot(index, vocab_size=vocab_size):\n","    index = np.asarray(index)\n","    if len(index.shape) == 0: # when it's a scale convert to a vector of size 1\n","        index = np.asarray([index])\n","    assert len(index.shape) == 1\n","    num = index.shape[0]\n","    output = np.zeros((num, vocab_size), dtype=np.float32)\n","    output[np.arange(num), index] = 1\n","    return output\n","\n","def one_hot_if_needed(label, vocab_size=vocab_size):\n","    label = np.asarray(label)\n","    if len(label.shape) <= 1:\n","        label = one_hot(label, vocab_size)\n","    assert len(label.shape) == 2\n","    return label\n","\n","# Using vectors of noise seeds and category labels, generate images\n","def sample(sess, noise, label, truncation=1., batch_size=8, vocab_size=vocab_size):\n","    noise = np.asarray(noise)\n","    label = np.asarray(label)\n","    num = noise.shape[0]\n","    if len(label.shape) == 0:\n","        label = np.asarray([label] * num)\n","    if label.shape[0] != num:\n","        raise ValueError('Got # noise samples ({}) != # label samples ({})'\n","                         .format(noise.shape[0], label.shape[0]))\n","    label = one_hot_if_needed(label, vocab_size)\n","    ims = []\n","    for batch_start in range(0, num, batch_size):\n","        s = slice(batch_start, min(num, batch_start + batch_size))\n","        feed_dict = {input_z: noise[s], input_y: label[s], input_trunc: truncation}\n","        ims.append(sess.run(output, feed_dict=feed_dict))\n","    ims = np.concatenate(ims, axis=0)\n","    assert ims.shape[0] == num\n","    ims = np.clip(((ims + 1) / 2.0) * 256, 0, 255)\n","    ims = np.uint8(ims)\n","    return ims\n","\n","def interpolate(a, b, num_interps):\n","    alphas = np.linspace(0, 1, num_interps)\n","    assert a.shape == b.shape, 'A and B must have the same shape to interpolate.'\n","    return np.array([(1-x)*a + x*b for x in alphas])\n","\n","def interpolate_and_shape(a, b, steps):\n","    interps = interpolate(a, b, steps)\n","    return (interps.transpose(1, 0, *range(2, len(interps.shape))).reshape(steps, -1))"]},{"cell_type":"markdown","metadata":{},"source":["### Create a TensorFlow Session and initialize Variables"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:13:04.232515Z","iopub.status.busy":"2022-01-16T15:13:04.231722Z","iopub.status.idle":"2022-01-16T15:13:07.322110Z","shell.execute_reply":"2022-01-16T15:13:07.321280Z","shell.execute_reply.started":"2022-01-16T15:13:04.232445Z"},"trusted":true},"outputs":[],"source":["initializer = tf.compat.v1.global_variables_initializer()\n","sess = tf.compat.v1.Session()\n","sess.run(initializer)"]},{"cell_type":"markdown","metadata":{},"source":["### Create Video of Interpolated BigGAN Generator Samples"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:13:09.760544Z","iopub.status.busy":"2022-01-16T15:13:09.760256Z","iopub.status.idle":"2022-01-16T15:14:06.192655Z","shell.execute_reply":"2022-01-16T15:14:06.191684Z","shell.execute_reply.started":"2022-01-16T15:13:09.760507Z"},"trusted":true},"outputs":[],"source":["# Category options: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n","category = 947 # mushroom\n","\n","# Important parameter that controls how much variation there is\n","truncation = 0.2 # reasonable range: [0.02, 1]\n","\n","seed_count = 10\n","clip_secs = 36\n","\n","seed_step = int(100 / seed_count)\n","interp_frames = int(clip_secs * 30 / seed_count)  # interpolation frames\n","\n","cat1 = category\n","cat2 = category\n","all_imgs = []\n","\n","for i in range(seed_count):\n","    seed1 = i * seed_step # good range for seed is [0, 100]\n","    seed2 = ((i+1) % seed_count) * seed_step\n","    \n","    z1, z2 = [truncated_z_sample(truncation, seed) for seed in [seed1, seed2]]\n","    y1, y2 = [one_hot([category]) for category in [cat1, cat2]]\n","\n","    z_interp = interpolate_and_shape(z1, z2, interp_frames)\n","    y_interp = interpolate_and_shape(y1, y2, interp_frames)\n","\n","    imgs = sample(sess, z_interp, y_interp, truncation=truncation)\n","    \n","    all_imgs.extend(imgs[:-1])\n","\n","# Save the video for displaying in the next cell, this is way more space efficient than the gif animation\n","imageio.mimsave('gan.mp4', all_imgs, fps=30)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-01-16T15:14:19.698043Z","iopub.status.busy":"2022-01-16T15:14:19.697764Z","iopub.status.idle":"2022-01-16T15:14:19.704712Z","shell.execute_reply":"2022-01-16T15:14:19.704001Z","shell.execute_reply.started":"2022-01-16T15:14:19.698015Z"},"trusted":true},"outputs":[],"source":["%%HTML\n","<video autoplay loop>\n","  <source src=\"gan.mp4\" type=\"video/mp4\">\n","</video>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":4}
